from pipeline_state import PipelineState
from print_to_log import print_to_log
import pandas as pd
import numpy as np

class FastModelAgent:
    """Agent for automated ML pipeline execution using direct function calls"""
    
    def __init__(self):
        print_to_log("ðŸš€ FastModelAgent initialized with direct function call approach")
    
    def handle_fast_model_request(self, state: PipelineState, target_column: str = None) -> PipelineState:
        """Handle fast model request with target column setting"""
        print_to_log("ðŸš€ FastModelAgent: Starting automated pipeline")
        
        # Set target column if provided
        if target_column:
            state.target_column = target_column
            print_to_log(f"ðŸŽ¯ Target column set: {target_column}")
        
        # Check if target column is set
        if not state.target_column:
            state.last_response = f"""ðŸŽ¯ **Target Column Required**
            
Please specify your target column from: {', '.join(list(state.raw_data.columns)[:10])}{'...' if len(state.raw_data.columns) > 10 else ''}

Reply with the target column name (e.g., 'f_segment')"""
            return state
        
        # Run the direct automated pipeline
        return self._run_automated_pipeline_direct(state)
    
    def _run_automated_pipeline_direct(self, state: PipelineState) -> PipelineState:
        """Run automated ML pipeline using direct function calls - bypasses agent wrapper issues"""
        print_to_log(f"ðŸš€ Starting DIRECT automated ML pipeline for target: {state.target_column}")
        
        try:
            # Import direct functions from implementations
            from preprocessing_agent_impl import (
                apply_outliers_treatment, 
                apply_missing_values_treatment,
                apply_encoding_treatment, 
                apply_transformations_treatment,
                detect_and_handle_extreme_outliers
            )
            from feature_selection_agent_impl import DataProcessor
            
            # Get original chat session for progress messages
            original_chat_session = state.chat_session
            
            # Send progress messages directly to Slack
            def send_progress(message: str):
                if original_chat_session:
                    try:
                        # Use correct import path for slack manager
                        from toolbox import slack_manager
                        if slack_manager and hasattr(slack_manager, 'send_message'):
                            slack_manager.send_message(original_chat_session, message)
                    except Exception as e:
                        print_to_log(f"âš ï¸ Could not send progress message: {e}")
                print_to_log(message)
            
            # Start preprocessing with progress messages
            send_progress("ðŸ§¹ **Started preprocessing**")
            
            # Initialize working dataframe
            df_working = state.raw_data.copy()
            
            # Phase 1: Overview & Analysis
            send_progress("ðŸ“Š **Starting overview phase**")
            print_to_log("ðŸ“Š Phase 1: Overview - Analyzing dataset for recommendations")
            send_progress("âœ… **Finished overview phase**")
            
            # Phase 2: Outliers
            send_progress("ðŸš¨ **Starting outlier phase**")
            print_to_log("ðŸš¨ Phase 2: Outliers - Detecting and handling extreme outliers")
            
            # Handle extreme outliers first
            df_working, extreme_report = detect_and_handle_extreme_outliers(df_working)
            print_to_log(f"âœ… Extreme outliers handled: {extreme_report.get('total_extreme_outliers', 0)} found")
            
            send_progress("âœ… **Finished outlier phase**")
            
            # Phase 3: Missing Values  
            send_progress("ðŸ—‘ï¸ **Starting missing values phase**")
            print_to_log("ðŸ—‘ï¸ Phase 3: Missing Values - Imputing missing data")
            
            # Create basic missing value recommendations for columns with missing data
            missing_recommendations = {}
            for col in df_working.columns:
                if df_working[col].isnull().sum() > 0:
                    if df_working[col].dtype in ['int64', 'float64']:
                        missing_recommendations[col] = {'treatment': 'median'}
                    else:
                        missing_recommendations[col] = {'treatment': 'mode'}
            
            # Apply missing value treatments
            if missing_recommendations:
                df_working = apply_missing_values_treatment(df_working, missing_recommendations)
                print_to_log(f"âœ… Missing values handled for {len(missing_recommendations)} columns")
            
            send_progress("âœ… **Finished missing values phase**")
            
            # Phase 4: Encoding
            send_progress("ðŸ·ï¸ **Starting encoding phase**")
            print_to_log("ðŸ·ï¸ Phase 4: Encoding - Converting categorical variables")
            
            # Create encoding recommendations for categorical columns
            encoding_recommendations = {}
            for col in df_working.columns:
                if col == state.target_column:
                    continue  # Skip target column
                    
                if df_working[col].dtype == 'object' or df_working[col].dtype.name == 'category':
                    unique_count = df_working[col].nunique()
                    unique_ratio = unique_count / len(df_working)
                    
                    if unique_ratio > 0.25:  # High cardinality - skip
                        encoding_recommendations[col] = {'strategy': 'skip'}
                        print_to_log(f"ðŸ”§ Skipping high cardinality column: {col} ({unique_count} unique values)")
                    elif 'date' in col.lower() or 'time' in col.lower():  # Date columns - skip
                        encoding_recommendations[col] = {'strategy': 'skip'}
                        print_to_log(f"ðŸ”§ Skipping date column: {col}")
                    elif unique_count <= 10:  # Low cardinality - label encoding
                        encoding_recommendations[col] = {'strategy': 'label_encoding'}
                        print_to_log(f"ðŸ”§ Label encoding: {col} ({unique_count} categories)")
                    else:  # Medium cardinality - one-hot with top categories
                        encoding_recommendations[col] = {'strategy': 'onehot_encoding'}
                        print_to_log(f"ðŸ”§ One-hot encoding: {col} ({unique_count} categories)")
            
            # Apply encoding treatments
            if encoding_recommendations:
                df_working = apply_encoding_treatment(df_working, encoding_recommendations)
                encoded_count = len([r for r in encoding_recommendations.values() if r['strategy'] != 'skip'])
                print_to_log(f"âœ… Encoding applied to {encoded_count} columns")
            
            send_progress("âœ… **Finished encoding phase**")
            
            # Phase 5: Transformations
            send_progress("ðŸ”„ **Starting transformations phase**")
            print_to_log("ðŸ”„ Phase 5: Transformations - Applying feature transformations")
            
            # Create transformation recommendations for skewed numeric columns
            transformation_recommendations = {}
            for col in df_working.columns:
                if col == state.target_column:
                    continue
                    
                if df_working[col].dtype in ['int64', 'float64']:
                    # Check for skewness
                    try:
                        skewness = df_working[col].skew()
                        if abs(skewness) > 1.0:  # Highly skewed
                            if df_working[col].min() >= 0:  # Non-negative values
                                transformation_recommendations[col] = {'transformation': 'log1p'}
                                print_to_log(f"ðŸ”§ Log transform: {col} (skewness: {skewness:.2f})")
                            else:
                                transformation_recommendations[col] = {'transformation': 'standardize'}
                                print_to_log(f"ðŸ”§ Standardize: {col} (skewness: {skewness:.2f})")
                        elif abs(skewness) > 0.5:  # Moderately skewed
                            transformation_recommendations[col] = {'transformation': 'standardize'}
                            print_to_log(f"ðŸ”§ Standardize: {col} (skewness: {skewness:.2f})")
                    except Exception as e:
                        print_to_log(f"âš ï¸ Could not calculate skewness for {col}: {e}")
            
            # Apply transformation treatments
            if transformation_recommendations:
                df_working = apply_transformations_treatment(df_working, transformation_recommendations)
                print_to_log(f"âœ… Transformations applied to {len(transformation_recommendations)} columns")
            
            send_progress("âœ… **Finished transformations phase**")
            send_progress("ðŸŽ‰ **Finished preprocessing**")
            
            # Update state with processed data
            state.cleaned_data = df_working
            state.preprocessing_strategies = {
                'outliers': {},
                'missing_values': missing_recommendations, 
                'encoding': encoding_recommendations,
                'transformations': transformation_recommendations
            }
            
            print_to_log(f"âœ… All preprocessing completed: {df_working.shape}")
            
            # Phase 6: Feature Selection
            send_progress("ðŸ” **Started feature selection**")
            print_to_log("ðŸ” Phase 6: Feature Selection - Applying IV and correlation filters")
            
            try:
                # Initialize DataProcessor
                processor = DataProcessor()
                
                # Apply intelligent cleaning with correct parameter name
                clean_data = processor.load_and_clean_data(df_working, state.target_column)
                
                # Apply IV filter
                iv_filtered_data = processor.apply_iv_filter(
                    data=clean_data,
                    target_column=state.target_column,
                    threshold=0.02
                )
                
                # Apply correlation filter
                final_data = processor.apply_correlation_filter(
                    data=iv_filtered_data,
                    threshold=0.5
                )
                
                # Update state with selected features
                state.processed_data = final_data
                state.selected_features = [col for col in final_data.columns if col != state.target_column]
                
                print_to_log(f"âœ… Feature selection: {df_working.shape} â†’ {final_data.shape}")
                print_to_log(f"âœ… Selected {len(state.selected_features)} features")
                
            except Exception as e:
                print_to_log(f"âš ï¸ Feature selection failed: {e}, using all features")
                state.processed_data = df_working
                state.selected_features = [col for col in df_working.columns if col != state.target_column]
            
            send_progress("âœ… **Final features selected**")
            
            # Phase 7: Model Building
            send_progress("ðŸ¤– **Started modeling**")
            print_to_log("ðŸ¤– Phase 7: Model Building - Training classification model")
            
            try:
                # Build model using available features and target
                model_data = state.processed_data if state.processed_data is not None else state.cleaned_data
                
                if state.target_column in model_data.columns:
                    # Prepare data with proper type handling
                    X = model_data.drop(columns=[state.target_column])
                    y = model_data[state.target_column]
                    
                    # Remove high cardinality columns that shouldn't be in model
                    columns_to_drop = []
                    for col in X.columns:
                        if X[col].dtype == 'object':
                            unique_ratio = X[col].nunique() / len(X)
                            if unique_ratio > 0.25:  # High cardinality
                                columns_to_drop.append(col)
                                print_to_log(f"ðŸ”§ Dropping high cardinality column from model: {col}")
                    
                    if columns_to_drop:
                        X = X.drop(columns=columns_to_drop)
                    
                    # Convert remaining object columns to numeric
                    for col in X.columns:
                        if X[col].dtype == 'object':
                            try:
                                # Try to convert to numeric
                                X[col] = pd.to_numeric(X[col], errors='coerce')
                                # Fill any resulting NaNs with median
                                if X[col].isnull().sum() > 0:
                                    X[col] = X[col].fillna(X[col].median())
                            except:
                                # If conversion fails, use label encoding
                                from sklearn.preprocessing import LabelEncoder
                                le = LabelEncoder()
                                X[col] = le.fit_transform(X[col].astype(str))
                    
                    # Ensure target is numeric
                    if y.dtype == 'object':
                        from sklearn.preprocessing import LabelEncoder
                        le_target = LabelEncoder()
                        y = le_target.fit_transform(y)
                    
                    # Final check - ensure all columns are numeric
                    non_numeric_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()
                    if non_numeric_cols:
                        print_to_log(f"ðŸ”§ Dropping non-numeric columns: {non_numeric_cols}")
                        X = X.select_dtypes(include=[np.number])
                    
                    print_to_log(f"ðŸ”§ Final model data: {X.shape} features, {len(y)} samples")
                    
                    # Simple model training
                    from sklearn.ensemble import RandomForestClassifier
                    from sklearn.model_selection import train_test_split
                    from sklearn.metrics import accuracy_score
                    
                    # Split data
                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
                    
                    # Train model
                    model = RandomForestClassifier(n_estimators=100, random_state=42)
                    model.fit(X_train, y_train)
                    
                    # Evaluate
                    y_pred = model.predict(X_test)
                    accuracy = accuracy_score(y_test, y_pred)
                    
                    state.trained_model = model
                    state.model_metrics = {'accuracy': accuracy}
                    
                    print_to_log(f"âœ… Model trained successfully - Accuracy: {accuracy:.3f}")
                else:
                    print_to_log(f"âš ï¸ Target column {state.target_column} not found in processed data")
                    
            except Exception as e:
                print_to_log(f"âš ï¸ Model building failed: {e}")
                import traceback
                traceback.print_exc()
            
            send_progress("âœ… **Final modeling results completed**")
            
            # Generate final success message
            final_shape = state.processed_data.shape if state.processed_data is not None else state.cleaned_data.shape
            feature_count = len(state.selected_features) if state.selected_features else final_shape[1] - 1
            model_status = "âœ… Trained Successfully" if state.trained_model else "âš ï¸ Training Attempted"
            
            accuracy_text = ""
            if state.trained_model and hasattr(state, 'model_metrics') and 'accuracy' in state.model_metrics:
                accuracy_text = f" (Accuracy: {state.model_metrics['accuracy']:.1%})"
            
            state.last_response = f"""ðŸŽ‰ **Fast ML Pipeline Complete!**

ðŸŽ¯ **Target:** {state.target_column}
ðŸ“Š **Data Shape:** {state.raw_data.shape} â†’ {final_shape}
ðŸ” **Features:** {feature_count} selected
ðŸ¤– **Model:** {model_status}{accuracy_text}

âœ… **Completed Phases:**
â€¢ ðŸ“Š Overview - Dataset analyzed
â€¢ ðŸš¨ Outliers - Extreme outliers handled
â€¢ ðŸ—‘ï¸ Missing Values - Data imputed  
â€¢ ðŸ·ï¸ Encoding - Variables encoded
â€¢ ðŸ”„ Transformations - Features transformed
â€¢ ðŸ” Feature Selection - IV & correlation filters applied
â€¢ ðŸ¤– Model Building - Classification model trained

**All phases completed automatically - your model is ready!**"""

            print_to_log("ðŸŽ‰ DIRECT automated ML pipeline completed successfully!")
            return state
            
        except Exception as e:
            error_msg = f"Direct pipeline execution failed: {str(e)}"
            print_to_log(f"âŒ {error_msg}")
            import traceback
            traceback.print_exc()
            state.last_error = error_msg
            state.last_response = f"âŒ **Pipeline Error:** {error_msg}"
            return state


def fast_model_agent(state: PipelineState) -> PipelineState:
    """Main entry point for fast model agent - called by langgraph_pipeline.py"""
    print_to_log("ðŸš€ [Fast Model Agent] Starting fast model pipeline")
    
    agent = FastModelAgent()
    
    # Check if this is a target column response
    if state.target_column is None and state.user_query and state.raw_data is not None:
        # Check if user query looks like a column name
        if state.user_query.strip() in state.raw_data.columns:
            print_to_log(f"ðŸŽ¯ [Fast Model Agent] Setting target column: {state.user_query.strip()}")
            return agent.handle_fast_model_request(state, state.user_query.strip())
    
    # Otherwise start the normal flow
    print_to_log("ðŸš€ [Fast Model Agent] Starting automated pipeline")
    return agent.handle_fast_model_request(state)
